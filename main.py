# -*- coding: utf-8 -*-
"""main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VbSfd-_5S-Uuw4Rqy-atC0rQ6uK_hixo
"""

# main.py
import os
from langchain.document_loaders import TextLoader
from langchain.text_splitters import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import Ollama
from langchain.chains import RetrievalQA

def build_qa_system():
    print("Loading speech.txt ...")
    loader = TextLoader("speech.txt")
    documents = loader.load()

    print("Splitting text ...")
    splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    docs = splitter.split_documents(documents)

    print("Creating embeddings (MiniLM-L6-v2) ...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    print("Saving vectors to ChromaDB ...")
    vectordb = Chroma.from_documents(
        docs,
        embedding=embeddings,
        persist_directory="chroma_db"
    )
    vectordb.persist()

    print("Initializing Mistral 7B (Ollama) ...")
    llm = Ollama(model="mistral")

    print("Building RAG chain ...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectordb.as_retriever(),
        chain_type="stuff"
    )

    return qa_chain


def main():
    print("\n=== AmbedkarGPT Q&A System ===\n")
    qa = build_qa_system()

    while True:
        user_input = input("\nAsk a question (or type 'exit'): ")

        if user_input.lower() == "exit":
            print("Goodbye!")
            break

        answer = qa.run(user_input)
        print("\nAnswer:", answer)


if __name__ == "__main__":
    main()